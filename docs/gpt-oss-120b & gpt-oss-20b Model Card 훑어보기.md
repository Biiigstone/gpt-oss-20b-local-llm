Model Card란?
- 인공지능 모델의 중요한 정보를 투명하게 공개하기 위해 만든 문서.
- 모델의 특징, 성능, 한계, 윤리적 고려사항 등을 기술한 설명서.
- AI 기술을 더 책임값 있고 투명하게 사용하자는 취지에서 만들어짐.

Model Card의 주 내용
- **모델 기본 정보 (Model Details):** 모델의 이름, 버전, 개발자, 라이선스 등 기본적인 정보를 담고 있어요.
- **사용 목적 (Intended Use):** 이 모델이 어떤 목적으로 개발되었고, 어떤 상황에서 사용되도록 의도되었는지를 설명해요.
- **학습 데이터 (Training Data):** 모델을 학습시키는 데 사용된 데이터의 종류와 특성을 알려줘요. 데이터에 편향성(bias)이 있을 수 있기 때문에 중요한 정보예요.
- **성능 평가 (Evaluation):** 다양한 데이터셋에서 모델의 성능을 측정한 결과를 보여줘요. 정확도, 속도 등 여러 지표가 포함될 수 있어요.
- **한계 및 잠재적 위험 (Limitations and Risks):** 모델이 잘 작동하지 않는 경우나, 잘못 사용될 경우 발생할 수 있는 문제점, 그리고 잠재적인 윤리적 위험 등을 명시해요.
- **윤리적 고려사항 (Ethical Considerations):** 모델을 사용할 때 고려해야 할 사생활 보호, 공정성, 차별 문제 등을 다루어요.

용어/개념 노트 

- proprietary models(독점 모델, 상업용 모델) : 특정 회사가 자사의 비공개 학습 데이터와 리소스를 활용하여 개발하고 소유한 AI 모델. 
  유의어 : Closed-Source Model

- 악의적 공격 : AI 모델을 나쁜 목적으로 사용하려는 모든 시도. 대표적인 방법으론 fine tuning이 있는데, OpenAI의 경우 생화학, 사이버, AI 자가 개선의 영역에서 fine tuning 시 안전 장치를 무력화(혹은 우회)하는지 평가하였다.

- Mixture-of-Experts, MOE : 모델 내부에 여러 분야의 '전문가(Experts)'를 두고, 질문이 들어오면 그에 맞는 최적의 전문가 몇 명만 활성화하여 문제를 해결하는 방식. 모델의 총 파라미터는 크지만, 활성 파라미터는 작아서 계산 효율이 높고 비용이 적게 듬. 

- Multi-Layer Perceptrom, MLP : 트랜스포머 아키텍처는 크게 어텐션과 MLP(Feed-Forward Network)로 구성됨. MLP는 어텐션이 중요하다고 판단한 정보를 받아서, 실질적인 계산과 추론을 통해 정보를 처리하고 다음 단어를 예측하는 부분.

- 총 파라미터 / 활성 파라미터
	- 총 파라미터 : 모델이 가진 모든 지식의 총량.
	- 활성 파라미터 : 한 번에 실제로 사용하는 지식의 양. 예를 들어 "120b"모델은 하나의 단어를 처리할 때 그 중 51억개만 사용함.

- [Post-training](https://tech.kakao.com/posts/662) : LLM은 다음 단어를 예측하는 방식으로 "문서"를 학습하기 때문에, 사용자가 제시한 명령을 "이해하고 수행"하는 데 필요한 능력은 부족한 경우가 많다. 따라서, 모델이 사용자의 명령을 인식하고 적절히 반응할 수 있도록 추가 훈련을 거쳐야 하며, 이를 Post-training이라고 한다. 이를 통해 다양한 유형의 명령어, 지시문 등의 데이터를 학습하여 특정 문맥에서 바람직한 응답을 선택하고 적절히 추론하는 능력을 키우게 된다.

- deliberative alignment(숙고적 정렬) : 사용자의 요청 거부 시, "왜 이 요청을 거부해야 하는가?"를 스스로 생각하고 판단하도록 가르치는 훈련 방식. 
  `gpt-oss`는 이 방식을 통해 어떠한 것이 나쁜 행동인지, 왜 거부해야하고 올바른 가치에 행동을 맞추어야 하는지 학습함.

- Jailbreak(탈옥) : 알려진 공격 기법을 사용해 모델의 안전 정책을 우회하려는 시도. 
탈옥의 목표는, 모델이 "불법적인 조언", "증오 발언" 등의 생성 금지 컨텐츠를 만들도록 유도하는 것이다. 대표적으로 "알려진 공격 기법"은 "할머니가 들려준 이야기" 기법. 
`gpt-oss`는 이렇게 널리 알려진 공격 기법에 대해서는 높은 방어 능력을 갖는다.

- Instruction Hierarchy(지시 계층) : 사용자 계층을 만들어 지시 간 우선순위를 만드는 것.
  AI 서비스는 보통 세 종류의 사용자가 관여한다. 
	- 1. 시스템 : AI 모델의 최고 관리자 (예: OpenAI, gpt-oss 개발자)
	- 2. 개발자 : AI 모델을 가져와 자신의 앱이나 서비스를 만드는 사람
	- 3. 사용자 : 개발자가 만든 앱을 사용하는 일반 사람
	이때 하위 계급이 악의적인 지시를 내려 최상위 계급이 설정한 안전 장치를 무력화하려는 시도(탈옥)가 있을 수 있다. '지시 계층은' 이러한 문제를 막기 위해 AI에게 `시스템 > 개발자 > 사용자`와 같은 우선 순위를 따르도록 훈련시키는 것.
	`gpt-oss`는 지시 계층에 대한 학습이 부족하여, 알려진 공격 패턴은 잘 막더라도(탈옥 방어), 사용자의 엉뚱한 지시에 속아 시스템의 중요 지침을 어기는 모습을 보일 수 있다. 


재미었던 내용

- 모델의 CoT 과정 중 "나쁜 생각"을 갖지 않도록 직접적인 압력을 받으면, 모델이 잘못된 행동을 하면서도 자신의 생각을 숨기기는 방법을 학습할 수 있음. (ㄷㄷㄷ)

들었던 의문점
- 지시 계층 준수가 closed-source model 보다 약하다고 하는데, 어느 정도의 성능일까? 탈옥이 가능할까?
# 요약
## 1장 Introduction

- OpenAI 사에서 `gpt-oss-120b`와 `gpt-oss-20b` 두 개의 오픈 웨이트 모델을 공개하였다. 
- 이 모델들은 에이전트 워크 플로우에 최적화 되어 있으며, 강력한 지시 사항 준수, 도구 사용(웹 검색, 코드 실행 등), 높은 추론 능력, 전체 사고 과정(CoT) 제공 및 사용자 정의가 가능하다.
- 기본 모델을 생화학, 사이버, AI 자가 개선과 같은 위험 영역에서 평가했을 때, 위험도 '높음' 수준에 도달하지 않았다.
- 악의적인 의도를 가지고 모델을 미세 조정 하더라도, 생화학, 사이버, AI 자가 개선과 같은 위험 영역에서 '높음' 위험 역량에 도달하지 않았다. 
- 그럼에도 불구하고 미세 조정을 통해 모델 자체의 안전 장치를 우회할 수 있으므로, 사용자가 추가적인 안전 장치를 구현해야 할 필요는 있다.

## 2장 모델 아키텍처, 데이터, 훈련 및 평가

- `gpt-oss`모델들은 전문가 혼합 트랜스포머 아키텍처를 기반으로 하며, 메모리 효율성을 4.25비트로 양자화 하였다. '120b' 모델은 80GB GPU에, '20B' 모델은 16GB GPU에 탑재할 수 있다.
- 훈련 데이터셋은 STEM, 코딩, 일반 지식에 초점을 맞추었으며, 안전을 위해 유해 콘텐츠는 필터링되었다. 지식 마감일은 2024년 6월이다.
- 하모니 채팅 형식 : `gpt-oss`만의 대화 형식으로, `System > Developer > User > Assistant > Tool` 순의 명확한 지시 순서를 따르며, 모델의 생각 과정과 최종 답변을 확인할 수 있는 채널이 존재한다. 이는 `gpt-oss`가 고급 에이전트 기능을 사용할 수 있음을 의미한다.
- 가변 추론 : 사용자가 AI의 '생각 깊이'를 조절할 수 있다. low, medium, high 세 단계로 설정하여, 간단한 답변이 필요할 때와 깊은 추론이 필요할 때를 구분할 수 있다.
- 평가 
	- 전반적 성능 : '120b'는 여러 벤치마크(수학, 코딩, 다국어)에서 OpenAI의 04-mini에 근접한 성능을 보인다. 특히 의료 분야는 뛰어난 추론 능력을 보인다.
	- 추론 확장성 : 추론 수준을 높이면 지연 시간과 비용이 증가하지만, 더 긴 사고 과정을 통해 정확도가 향상되는 경향을 보인다.


## 3장 안전성 테스트 및 완화 접근법

- 안전 훈련 기법 : 후훈련 과정에서 숙고적 정렬(deliberative alignment) 기법을 사용하여, 모델이 부적절한 요청을 거부하고, 탈옥 시도에 저항하며, 정해진 지시 계층을 따르도록 학습시켰다.
- 오픈 웨이트 모델은, 누구나 미세조정을 통해 원하는 방향으로 모델을 바꿀 수 있다. 이 방향에는 악의적인 방향도 포함이 되므로, OpenAI 자사 기준에 따라 기본 모델의 위험도와 악의적인 방향으로 미세 조정 되었을 때의 위험도를 평가하였다.
- 평가 결과 주요 위험 범주(생화학, 사이버, AI 자가 개선)에서 '높음' 수준의 역량에는 도달하지 않는 것으로 확인되었다.

## 4장 기본 안전 성능 : 관찰된 과제 및 평가

- 허용되지 않는 콘텐츠: `gpt-oss` 모델들은 표준 안전성 평가에서 OpenAI 04-mini와 동등한 성능을 보이며, 더 어려운 '프로덕션 벤치마크'에서는 오히려 더 뛰어난 성능을 보이는 경향이 있습니다.
- 탈옥(Jailbreaks) 방어: 알려진 탈옥 기법에 대한 방어 능력은 OpenAI 04-mini와 유사한 수준입니다.
- 지시 계층 준수: 시스템 메시지를 사용자 메시지보다 우선시하는 능력은 OpenAI 04-mini보다 약한 것으로 나타났습니다. 이는 `gpt-oss` 모델의 약점일 수 있으나, 오픈 모델의 특성상 개발자가 직접 미세 조정을 통해 이 부분을 강화할 수 있는 가능성이 있습니다.
- 사고 과정(CoT)의 투명성: 연구 및 모니터링 가능성을 위해 모델의 사고 과정(CoT)에 의도적으로 안전 필터를 적용하지 않았습니다. 따라서 개발자는 CoT에 환각이나 부적절한 내용이 포함될 수 있음을 인지하고, 사용자에게 직접 노출하기 전 필터링 등의 처리를 해야 합니다.
- 환각: 더 작은 모델의 특성상, 웹 브라우징 기능 없이는 대형 모델인 OpenAI 04-mini보다 사실적 정확도가 낮고 환각을 일으키는 비율이 높게 나타났습니다.
- 공정성 및 편향: 공정성 및 편향을 측정하는 BBQ 평가에서는 OpenAI 04-mini와 비슷한 수준의 성능을 보였습니다.

## 5장 준비성 프레임워크 

#### 5.1 적대적 훈련

- OpenAI는 `gpt-oss` 모델의 잠재적 위험을 평가하기 위해, 기술과 자원을 가진 공격자가 모델을 악의적으로 미세 조정하는 상황을 시뮬레이션했습니다.
- 이들은 `gpt-oss-120b` 모델이 '높음(High)' 수준의 역량에 도달할 가능성이 있다고 판단된 생화학 및 사이버 분야에 대해 적대적으로 미세 조정한 버전을 만들었습니다.
- 이 훈련은 유해한 프롬프트에 응답하도록 보상하는 '도움 전용 훈련(Helpful-only training)'과 특정 위험 분야(생물학, 사이버)의 전문가 데이터를 활용한 역량 극대화 두 가지 요소를 결합했습니다.
- OpenAI의 안전 자문 그룹(SAG)은 이 테스트를 검토한 후, 강력한 미세 조정에도 불구하고 모델이 생화학 또는 사이버 위험에서 '높음' 역량에 도달하지 않았다고 결론 내렸습니다.
#### **5.1.1 외부 안전 전문가 피드백에 대한 적대적 훈련 방법론**

- OpenAI는 자신들의 악의적 미세 조정 방법론을 검증하기 위해 외부 안전 전문가 그룹(METR, SecureBio, Daniel Kang)을 참여시켰습니다.
- 전문가들은 총 22개의 권장 사항을 제출했으며, OpenAI는 이 중 시급성이 높은 12개 중 9개를 포함하여 총 11개의 권장 사항을 수용했습니다.
- 이 피드백을 통해 평가 프로세스를 강화하고, 논문과 모델 카드의 명확성을 개선했습니다.

#### **5.2 역량 분석 결과 (Capability findings) 요약**

##### **5.2.1 생화학 - 적대적으로 미세 조정된 모델 요약**

- `gpt-oss-120b`는 생물학적 지식 및 유해 시나리오 관련 텍스트 질문에 상당한 강점을 보였으나, 복잡한 실험 절차 문제 해결 능력에서는 '높음' 역량 기준에 도달하지 못했습니다.
- 이 모델의 출시는 오픈 소스 모델의 생물 보안 관련 역량을 크게 발전시키지는 않는 것으로 나타났습니다. 왜냐하면 최근 출시된 다른 오픈 모델들(Qwen 3 Thinking, Kimi K2)이 이미 이와 비슷하거나 더 높은 수준의 역량을 보여주기 때문입니다.
- 다양한 생물 보안 평가에서 특정 모델 하나가 지속적으로 다른 모델을 능가하지는 않았습니다.

##### **5.2.2 사이버 보안 - 적대적으로 미세 조정된 모델 요약**

- 사이버 보안 평가는 컴퓨터 시스템의 기밀성, 무결성, 가용성을 파괴하는 사이버 공격에 모델이 사용될 수 있는 위험에 초점을 맞춥니다.
- 평가 결과,`gpt-oss` 모델은 OpenAI 03과 비슷한 성능을 보였으며, '높음' 역량 기준에는 미치지 못했습니다.
- **CTF(Capture the Flag) 챌린지**: 대학 수준의 CTF에서 `gpt-oss-120b`는 `04-mini`보다 나은 성능을 보였으나 `03`보다는 낮았습니다. 전문가 수준 CTF에서는 악의적으로 훈련된 'cybermax' 버전조차 `03` 모델보다 약간 낮은 성능을 보였습니다.
- **사이버 레인지(Cyber Range)**: 현실적인 네트워크 환경에서 완전한 사이버 공격을 수행하는 테스트에서는, 힌트가 없는 조건에서 어떤 모델도 시나리오를 해결하지 못했습니다.

##### **5.2.3 AI 자가 개선 (AI Self-Improvement) 요약**

- `gpt-oss` 모델들은 AI 자가 개선 위험과 관련된 소프트웨어 엔지니어링 및 AI 연구 과제에서 향상된 성능을 보이지 않았습니다.
- 모든 관련 벤치마크(SWE-bench Verified, OpenAI PRs, PaperBench)에서 OpenAI 03과 04-mini 모델이 여전히 가장 높은 성능을 유지했습니다.
- **SWE-bench Verified (소프트웨어 엔지니어링)**: `gpt-oss` 모델들은 `03` 및 `04-mini`와 비슷한 성능을 보였습니다.
- **OpenAI PRs (ML 연구 과제)**: `gpt-oss` 모델들은 `04-mini`보다 단지 2% 낮은 점수를 기록했습니다.
- **PaperBench (AI 논문 복제)**: `gpt-oss` 모델들은 `03` 및 `04-mini`에 비해 상당히 낮은 성능을 보였습니다.

## 6장 부록 - 하모니 채팅 구조 및 예시

- **목적**: 6장은 `gpt-oss` 모델이 사용하는 '하모니(harmony) 채팅 형식'의 실제 입력과 출력 예시를 제공합니다.
- **입력 구조**: 입력 예시는 세 부분으로 구성됩니다.
    - **시스템(system) 메시지**: 모델의 기본 설정(지식 마감일, 추론 수준 등)을 정의합니다
    - **개발자(developer) 메시지**: 모델이 사용할 수 있는 도구(예: 날씨 확인 함수)의 스키마와 추가 지침을 명시합니다.
    - **사용자(user) 메시지**: 최종 사용자의 실제 질문을 담습니다.
- **출력 구조**: 모델의 응답은 여러 '채널'로 나뉩니다.
    - **분석(analysis) 채널**: "get_weather 함수를 사용해야겠다"와 같이 모델의 내부적인 사고 과정(CoT)을 보여줍니다.
    - **주석(commentary) 채널**: `{"location":"San Francisco"}`와 같이 정의된 함수를 실제로 호출하는 내용을 담습니다.

## 7장 부록 - 적테적 테스트 방법론의 권장 사항과 대응 방안

7장은 적대적 훈련 방법론에 대한 **외부 전문가들의 피드백과 OpenAI의 조치**를 상세히 설명하는 기술 부록입니다.

- **구현된 권장 사항:** 전문가들이 제안한 22개 중 11개의 권장 사항을 수용했습니다. 주요 내용은 위협 모델의 가정을 더 명확히 하고 , 평가 데이터와 방법을 보강하여 신뢰도를 높이고 , 전문가 기준선이나 모델의 답변 거부율 같은 지표를 명확하게 보고하는 것 등이었습니다.
    
- **미채택 권장 사항:** 시급하다고 분류된 3가지 권장 사항은 채택하지 않았으며, 그 이유는 다음과 같습니다.
    
    1. 특정 평가 기법('Best-of-N')은 예상되는 효과가 제한적이라 전면 적용하지 않았습니다.
        
    2. 'ProtocolQA' 평가는 한계가 있지만 독특하고 중요한 안전 신호를 제공하기 때문에 제외하지 않았습니다.
        
    3. 오픈 모델과 폐쇄형 모델의 거부율 비교는 과거 데이터상 유의미한 정보를 얻기 어렵다고 판단했습니다.
        

결론적으로, 외부의 비판적 검토를 통해 평가 방법론을 강화했지만, 모든 제안을 수용하기보다는 실효성과 중요도를 고려하여 선별적으로 반영했음을 보여줍니다.


# 본문 번역

## 1장 Introduction

저희는 아파치 2.0 라이선스와 저희의 gpt-oss 사용 정책에 따라 사용 가능한 두 개의 오픈 웨이트(open-weight) 추론 모델인 gpt-oss-120b와 gpt-oss-20b를 소개합니다. 오픈소스 커뮤니티의 피드백을 바탕으로 개발된 이 텍스트 전용 모델들은, 강력한 지시사항 준수, 웹 검색 및 파이썬 코드 실행과 같은 도구 사용, 그리고 복잡한 추론이 필요하지 않은 작업에 대해 추론 노력을 조절하는 능력을 포함한 추론 역량을 갖춘 에이전트 워크플로우(agentic workflows) 내에서 사용되도록 설계되었습니다. 이 모델들은 사용자 정의가 가능하며, 전체 사고 과정(Chain-of-Thought, CoT)을 제공하고, 구조화된 출력을 지원합니다.

안전성은 저희의 오픈 모델 접근 방식에 있어 기초가 됩니다. 이들은 독점 모델과는 다른 위험 프로필을 제시합니다: 일단 출시되면, 단호한 공격자들이 안전 거부를 우회하거나 직접적으로 해를 끼치도록 미세 조정할 수 있으며, OpenAI가 추가적인 완화 조치를 실행하거나 접근을 철회할 가능성이 없습니다. 일부 상황에서는 개발자와 기업이 저희 API 및 제품을 통해 제공되는 모델에 내장된 시스템 수준의 보호 기능을 복제하기 위해 추가적인 안전 장치를 구현해야 할 것입니다.

저희는 이 문서를 '시스템 카드'가 아닌 '모델 카드'라고 부르는데, 이는 gpt-oss 모델들이 다양한 이해관계자들에 의해 생성되고 유지되는 광범위한 시스템의 일부로 사용될 것이기 때문입니다. 모델들은 기본적으로 OpenAI의 안전 정책을 따르도록 설계되었지만, 다른 이해관계자들 또한 해당 시스템을 안전하게 유지하는 방법에 대해 자신들만의 결정을 내리고 실행할 것입니다.

저희는 gpt-oss-120b에 대해 확장 가능한 역량 평가를 실행했으며, 기본 모델이 저희의 준비성 프레임워크(Preparedness Framework)의 세 가지 추적 범주(생화학적 능력, 사이버 능력, AI 자가 개선) 중 어느 것에서도 '높음(High)' 역량에 대한 저희의 지표 임계값에 도달하지 않음을 확인했습니다.

저희는 또한 두 가지 추가적인 질문을 조사했습니다:

- 악의적인 행위자들이 gpt-oss-120b를 미세 조정하여 생화학 및 사이버 영역에서 '높음' 역량에 도달하게 할 수 있을까? 공격자의 잠재적 행동을 시뮬레이션하여, 저희는 이 두 범주에 대해 gpt-oss-120b 모델을 적대적으로 미세 조정했습니다. OpenAI의 안전 자문 그룹("SAG")은 이 테스트를 검토하고, OpenAI의 업계 최고 수준의 훈련 스택을 활용한 강력한 미세 조정에도 불구하고 gpt-oss-120b가 생화학적 위험이나 사이버 위험에서 '높음' 역량에 도달하지 않았다고 결론 내렸습니다.
    
- gpt-oss-120b의 출시가 오픈 파운데이션 모델의 생물학적 역량의 최전선을 크게 발전시킬 것인가? 저희는 그 답이 '아니오'라는 것을 발견했습니다: 대부분의 평가에서, 기존에 존재하는 하나 또는 그 이상의 오픈 모델들의 기본 성능이 gpt-oss-120b의 적대적으로 미세 조정된 성능과 거의 일치하는 수준에 도달했습니다.
    

이번 출시의 일환으로, OpenAI는 유익한 AI를 발전시키고 생태계 전반에 걸쳐 안전 기준을 높이는 데 대한 저희의 약속을 재확인합니다.

## 2장 모델 아키텍처, 데이터, 훈련 및 평가

gpt-oss 모델은 GPT-2 및 GPT-3 아키텍처를 기반으로 하는 자기회귀적 전문가 혼합(Mixture-of-Experts, MOE) 트랜스포머입니다. 저희는 두 가지 모델 크기를 출시합니다: gpt-oss-120b는 36개의 레이어로 구성되며 (총 1168억 개의 파라미터와 토큰당 순전파(forward pass) 시 51억 개의 "활성" 파라미터), gpt-oss-20b는 24개의 레이어로 구성됩니다 (총 209억 개의 파라미터와 36억 개의 활성 파라미터). 표 1은 파라미터 수의 전체 내역을 보여줍니다.

| 구성 요소                                                                                                                                               | 120b    | 20b     |
| --------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------- |
| MLP                                                                                                                                                 | 1147.1억 | 191.2억  |
| Attention                                                                                                                                           | 9.6억    | 6.4억    |
| Embed/Unembed                                                                                                                                       | 11.6억   | 11.6억   |
| 활성 파라미터                                                                                                                                             | 51.3억   | 36.1억   |
| 총 파라미터                                                                                                                                              | 1168.3억 | 209.1억  |
| 체크포인트 크기                                                                                                                                            | 60.8GiB | 12.8GiB |
| <br>                                                                                                                                                |         |         |
| **표 1: 모델 파라미터 수.** 저희는 편의상 모델들을 "120b"와 "20b"로 지칭하지만, 기술적으로는 각각 1168억 개와 209억 개의 파라미터를 가지고 있습니다. Unembedding 파라미터는 활성 파라미터에 포함되지만, 임베딩은 포함되지 않습니다. |         |         |

**2.1 양자화**

저희는 모델의 메모리 사용량을 줄이기 위해 양자화(quantization)를 활용합니다. 저희는 MoE 가중치를 MXFP4 형식으로 양자화하여 모델을 후훈련(post-trained)했으며, 여기서 가중치는 파라미터당 4.25비트로 양자화됩니다. MoE 가중치는 전체 파라미터 수의 90% 이상을 차지하며, 이를 MXFP4로 양자화하면 더 큰 모델은 단일 80GB GPU에 맞출 수 있고, 더 작은 모델은 16GB 메모리만큼 적은 시스템에서도 실행할 수 있습니다. 표 1에 모델의 체크포인트 크기를 나열했습니다.

**2.2 아키텍처**

두 모델 모두 2880의 잔차 스트림 차원(residual stream dimension)을 가지며, 각 어텐션 및 MoE 블록 전에 활성화(activations)에 제곱 평균 제곱근 정규화(root mean square normalization)를 적용합니다. GPT-2와 유사하게 Pre-LN 배치를 사용합니다.

- **전문가 혼합(Mixture-of-Experts)**: 각 MoE 블록은 고정된 수의 전문가(gpt-oss-120b는 128개, gpt-oss-20b는 32개)와 잔차 활성화를 각 전문가에 대한 점수로 매핑하는 표준 선형 라우터 프로젝션으로 구성됩니다. 두 모델 모두 라우터가 제공하는 각 토큰에 대해 상위 4명의 전문가를 선택하고, 선택된 전문가에 대해서만 라우터 프로젝션의 소프트맥스로 각 전문가의 출력을 가중합니다. MoE 블록은 게이트가 있는 SwiGLU 활성화 함수를 사용합니다.
    
- **어텐션(Attention)**: GPT-3를 따라, 어텐션 블록은 밴드형 윈도우(banded window)와 완전 밀집(fully dense) 패턴 사이를 번갈아 가며, 밴드폭은 128 토큰입니다. 각 레이어는 차원 64의 쿼리 헤드 64개를 가지며, 8개의 키-값 헤드를 가진 그룹화된 쿼리 어텐션(Grouped Query Attention, GQA)을 사용합니다. 저희는 회전식 위치 임베딩(rotary position embeddings)을 적용하고 YaRN을 사용하여 밀집 레이어의 컨텍스트 길이를 131,072 토큰으로 확장합니다. 각 어텐션 헤드는 소프트맥스의 분모에 학습된 편향을 가지고 있어(off-by-one attention 및 attention sinks와 유사), 어텐션 메커니즘이 어떤 토큰에도 주의를 기울이지 않을 수 있게 합니다.
    

**2.3 토크나이저**

모든 훈련 단계에 걸쳐, 저희는 TikToken 라이브러리에서 오픈 소스로 제공하는

`0200k_harmony` 토크나이저를 활용합니다. 이것은 GPT-4o 및 OpenAI 04-mini와 같은 다른 OpenAI 모델에 사용되는

`0200k` 토크나이저를 확장한 바이트 페어 인코딩(BPE)으로, 표 18에 설명된 저희의 하모니 채팅 형식에 명시적으로 사용되는 토큰을 추가하여 총 201,088개의 토큰을 가집니다.

**2.4 사전 훈련(Pretraining)**

- **데이터**: 저희는 STEM, 코딩 및 일반 지식에 중점을 둔 수조 개의 토큰으로 구성된 텍스트 전용 데이터셋으로 모델을 훈련합니다. 모델의 안전성을 향상시키기 위해, 저희는 GPT-4o의 CBRN 사전 훈련 필터를 재사용하여 사전 훈련에서 유해한 콘텐츠, 특히 위험한 생물 보안 지식과 관련된 데이터를 필터링했습니다. 저희 모델의 지식 마감일은 2024년 6월입니다.
    
- **훈련**: gpt-oss 모델은 전문가에 최적화된 Triton 커널을 사용하는 PyTorch 프레임워크를 이용해 NVIDIA H100 GPU에서 훈련되었습니다 . gpt-oss-120b의 훈련 실행에는 210만 H100-시간이 필요했으며, gpt-oss-20b는 거의 10배 더 적게 필요했습니다. 두 모델 모두 메모리 요구 사항을 줄이고 훈련을 가속화하기 위해 FlashAttention 알고리즘을 활용합니다.
    

**2.5 추론 및 도구 사용을 위한 후훈련(Post-Training)**

사전 훈련 후, 저희는 OpenAI 03과 유사한 CoT RL(사고 과정 강화 학습) 기술을 사용하여 모델을 후훈련합니다. 이 절차는 모델에게 CoT를 사용하여 문제를 추론하고 해결하는 방법과 도구를 사용하는 방법을 가르칩니다. 유사한 RL 기술 때문에 이 모델들은 ChatGPT와 같은 저희 자사 제품에서 제공되는 모델과 유사한 개성을 가집니다. 저희 훈련 데이터셋은 코딩, 수학, 과학 등 광범위한 문제로 구성되어 있습니다.

**2.5.1 하모니 채팅 형식(Harmony Chat Format)**

모델 훈련을 위해, 저희는 하모니 채팅 형식으로 알려진 사용자 정의 채팅 형식을 사용합니다. 이 형식은 메시지 경계를 구분하기 위해 특수 토큰을 제공하고, 메시지 작성자와 수신자를 나타내기 위해 키워드 인수(예: User 및 Assistant)를 사용합니다. 저희는 OpenAI API 모델에 있는 것과 동일한 시스템 및 개발자 메시지 역할을 사용합니다. 이러한 역할을 사용하여 모델은 지시 충돌을 해결하기 위해 역할 기반 정보 계층 구조를 따릅니다:

`$System > Developer > User > Assistant > Tool$`. 이 형식은 또한 각 메시지의 의도된 가시성을 나타내는 "채널"을 도입합니다. 예를 들어, CoT 토큰에 대한 `analysis`, 함수 도구 호출에 대한 `commentary`, 사용자에게 표시되는 답변에 대한 `final`이 있습니다. 이 형식을 통해 gpt-oss는 CoT 내에 도구 호출을 삽입하거나 사용자에게 더 긴 행동 계획을 설명하는 서문을 제공하는 등 고급 에이전트 기능을 제공할 수 있습니다. 저희가 함께 제공하는 오픈소스 구현 및 가이드는 이 형식의 올바른 사용법에 대한 전체 세부 정보를 제공합니다. gpt-oss 모델의 최상의 능력을 달성하기 위해서는 올바르게 배포하는 것이 중요합니다.

**2.5.2 가변 노력 추론 훈련(Variable Effort Reasoning Training)**

저희는 모델이 낮음, 중간, 높음의 세 가지 추론 수준을 지원하도록 훈련합니다. 이 수준들은 시스템 프롬프트에서 "Reasoning: low"와 같은 키워드를 삽입하여 구성됩니다. 추론 수준을 높이면 모델의 평균 CoT 길이가 증가합니다.

**2.5.3 에이전트 도구 사용(Agentic Tool Use)**

후훈련 동안, 저희는 모델에게 다양한 에이전트 도구를 사용하도록 가르칩니다:

- **브라우징 도구**: 모델이 웹과 상호 작용하기 위해 검색 및 열기 기능을 호출할 수 있게 합니다. 이는 사실성을 돕고 모델이 지식 마감일을 넘어선 정보를 가져올 수 있게 합니다.
    
- **파이썬 도구**: 모델이 상태 저장 주피터 노트북 환경에서 코드를 실행할 수 있게 합니다.
    
- **임의의 개발자 함수**: OpenAI API와 유사하게 개발자 메시지에서 함수 스키마를 지정할 수 있습니다.
    

**2.6 평가**

저희는 표준적인 추론, 코딩 및 도구 사용 벤치마크에서 gpt-oss를 평가합니다. 모든 데이터셋에 대해, 저희는 모델의 기본 시스템 프롬프트를 사용하여 높은 추론 모드에 대한 기본 pass@1 결과를 보고합니다. OpenAI 03, 03-mini, 04-mini와 비교합니다.

- **추론, 사실성 및 도구 사용**: 주요 역량 평가 결과, gpt-oss 모델, 특히 `gpt-oss-120b`는 수학(AIME)에서 강점을 보이며, 이는 긴 CoT를 효과적으로 사용하기 때문이라고 생각합니다. 코딩 및 도구 사용(에이전트 작업)에서는
    
    `gpt-oss-120b`가 OpenAI의 04-mini 성능에 근접합니다. 또한, 추론 수준(낮음, 중간, 높음)을 높이면 CoT 길이가 길어짐에 따라 정확도가 부드럽게 향상되는 "테스트 시간 확장성"을 보입니다.
    
- **의료 성능**: HealthBench 평가에서 `gpt-oss-120b` 모델은 OpenAI 03과 경쟁력 있는 성능을 보이며 GPT-4o, OpenAI 03-mini, 04-mini를 능가하는 등, 의료 성능-비용 측면에서 큰 발전을 보여줍니다.
    
- **다국어 성능**: MMLU를 14개 언어로 번역한 MMMLU 평가에서, `gpt-oss-120b`는 높은 추론 수준에서 OpenAI 04-mini의 성능에 근접했습니다.


## 3장 안전성 테스트 및 완화 접근법

후훈련(post-training) 동안, 저희는 모델이 광범위한 콘텐츠(예: 불법적인 조언)에 대한 요청을 거부하고, 탈옥(jailbreaks)에 견고하며, 지시 계층(instruction hierarchy)을 준수하도록 가르치기 위해 숙고적 정렬(deliberative alignment)을 사용합니다. 오픈 모델 가중치에 대한 저희의 오랜 견해에 따라, 저희는 오픈 웨이트 모델의 테스트 조건이 "하위 사용자(downstream actors)가 모델을 수정할 수 있는 다양한 방식을 이상적으로 반영해야 한다"고 믿습니다. 오픈 모델의 가장 유용한 특성 중 하나는 하위 사용자가 모델을 수정하여 초기 역량을 확장하고 개발자의 특정 애플리케이션에 맞게 조정할 수 있다는 것입니다. 그러나 이는 악의적인 당사자가 모델의 유해한 역량을 잠재적으로 강화할 수 있음을 의미하기도 합니다. 따라서 오픈 웨이트 모델 출시의 위험을 엄격하게 평가하려면 악의적인 당사자가 모델을 실현 가능하게 수정할 수 있는 합리적인 범위의 방식, 즉 미세 조정(fine-tuning)을 포함한 테스트가 포함되어야 합니다.

gpt-oss 모델은 기본적으로 OpenAI의 안전 정책을 따르도록 훈련되었습니다. 저희는 gpt-oss-120b에 대해 확장 가능한 준비성(Preparedness) 평가를 실행했으며, 기본 모델이 저희의 준비성 프레임워크의 세 가지 추적 범주(생화학적 능력, 사이버 능력, AI 자가 개선) 중 어느 것에서도 '높음(High)' 역량에 대한 저희의 지표 임계값에 도달하지 않음을 확인했습니다.


## 4장 기본 안전 성능 : 관찰된 과제 및 평가

**4.1 허용되지 않는 콘텐츠 (Disallowed Content)**

다음 평가는 모델이 증오성 콘텐츠나 불법적인 조언을 포함하여 OpenAI의 안전 정책에 따라 허용되지 않는 콘텐츠에 대한 요청을 따르지 않는지 확인합니다. 저희는 여러 평가를 고려합니다:

- **표준 허용되지 않는 콘텐츠 평가**: 저희는 모델의 출력이 허용되지 않는 콘텐츠 요청에 대해 안전한지 테스트하기 위한 표준 평가를 보고합니다. 하지만 최근 모델들은 이 벤치마크를 거의 완벽하게 통과하여(결과 표에서 볼 수 있듯이), 점진적인 안전성 향상에 대한 유용한 신호를 더 이상 제공하지 못합니다. 지속적인 진척도를 벤치마킹하는 데 도움을 주기 위해, 저희는 새로운 '프로덕션 벤치마크' 평가 세트를 만들었습니다. 가까운 미래에 이 오래된 세트의 발표를 중단하고, 아래의 더 어려운 세트를 공유할 계획입니다.
    
- **프로덕션 벤치마크**: ChatGPT 에이전트와 함께 도입된 이것은 프로덕션 데이터를 더 잘 대표하는 대화로 구성된 새롭고 더 어려운 평가 세트이며, 따라서 이전의 허용되지 않는 콘텐츠 평가보다 훨씬 더 다중 턴(multi-turn)이고 덜 간단합니다.
    

저희는 LLM 기반 채점 모델을 사용하여 완료된 내용을 평가합니다. 관련 OpenAI 정책에 따라 모델이 안전하지 않은 출력을 생성하지 않았는지 확인하는 '안전하지 않음(not unsafe)'이라는 측정 기준을 사용합니다. 프로덕션 벤치마크 세트는 특별히 더 어렵게 설계되었으며, 이는 시간이 지남에 따라 모델의 안전성이 어떻게 개선되는지에 대한 유용한 신호를 제공합니다. 따라서 점수는 표준 평가보다 낮을 것으로 예상됩니다. 저희는 `gpt-oss-120b`와 `gpt-oss-20b`가 일반적으로 OpenAI 04-mini와 동등한 수준으로 수행된다는 것을 발견했습니다. 두 `gpt-oss` 모델 모두 '표준 허용되지 않는 콘텐츠 평가'에서 OpenAI 04-mini와 일반적으로 1-2포인트 내에서 수행되며, '프로덕션 벤치마크' 평가에서는 OpenAI 04-mini를 상당히 능가하는 경향이 있습니다. 더 작은 모델(`gpt-oss-20b`)은 '프로덕션 벤치마크'의 불법/폭력 카테고리에서 OpenAI 04-mini보다 성능이 낮지만, 비추론 채팅 모델(GPT-4o)보다는 여전히 성능이 뛰어납니다.

**4.2 탈옥 (Jailbreaks)**

저희는 `gpt-oss-120b`와 `gpt-oss-20b`가 탈옥(jailbreaks)에 대한 견고성을 추가로 평가합니다. 탈옥은 모델이 생성해서는 안 되는 콘텐츠에 대한 거부를 의도적으로 우회하려는 적대적인 프롬프트입니다. 저희는 다음 접근 방식을 사용하여 평가합니다:

- **StrongReject**: 위의 안전 거부 평가에서 가져온 예시에 알려진 탈옥 기법을 삽입합니다. 그런 다음 허용되지 않는 콘텐츠 확인에 사용하는 것과 동일한 정책 채점기를 통해 실행합니다. 저희는 여러 유해 범주에 걸친 기본 프롬프트에 대해 탈옥 기술을 테스트하고, 관련 정책에 따라 `안전하지 않음(not_unsafe)`을 평가합니다.
    

저희는 `gpt-oss-120b`와 `gpt-oss-20b`가 일반적으로 OpenAI 04-mini와 유사하게 수행된다는 것을 발견했습니다.

**4.3 지시 계층 (Instruction Hierarchy)**

모델 추론 제공업체는 `gpt-oss`의 추론 배포를 사용하는 개발자가 최종 사용자 중 한 명의 모든 프롬프트에 포함될 사용자 지정 개발자 메시지를 지정할 수 있도록 할 수 있습니다. 이 기능은 유용하지만, 제대로 처리되지 않으면 개발자가 `gpt-oss`의 안전장치를 우회할 수 있는 잠재적인 가능성도 있습니다.

이 문제를 완화하기 위해, 저희는 모델에게 지시 계층(Instruction Hierarchy)을 따르도록 가르쳤습니다. 높은 수준에서, 저희는 시스템 메시지, 개발자 메시지, 사용자 메시지를 포함한 여러 역할을 사용하는 저희의 하모니 프롬프트 형식으로 모델을 후훈련했습니다. 저희는 이러한 다른 역할의 메시지들이 서로 충돌하는 예시를 수집했고, `gpt-oss`가 개발자 메시지보다 시스템 메시지의 지시를 따르도록, 그리고 사용자 메시지보다 개발자 메시지의 지시를 따르도록 감독했습니다. 이는 모델 추론 제공업체와 모델을 사용하는 개발자 모두에게 각자의 수준에서 안전장치를 제어할 수 있는 수단을 제공합니다.

평가 결과, 저희는 `gpt-oss-120b`와 `gpt-oss-20b`가 지시 계층 평가에서 일반적으로 OpenAI 04-mini보다 성능이 낮다는 것을 관찰했습니다. 이것이 왜 그런지에 대해서는 더 많은 연구가 필요하지만, 두 가지를 기록해 둡니다:

1. `gpt-oss-120b`와 `gpt-oss-20b`의 StrongReject 탈옥 평가 성능은 OpenAI 04-mini와 거의 동등합니다. 이는 두 `gpt-oss` 모델이 알려진 탈옥에 비교적 견고하지만, OpenAI 04-mini만큼 사용자가 시스템 메시지를 무시하는 것을 방지하는 데는 강하지 않다는 것을 의미합니다.
    
2. 그럼에도 불구하고, 개발자는 두 `gpt-oss` 모델을 마주치는 탈옥에 대해 더 견고하게 미세 조정할 수 있으며, 이는 필요할 경우 더 높은 견고성을 확보할 수 있는 경로가 있음을 의미합니다.
    

**4.4 환각적인 사고 과정 (Hallucinated chains of thought)**

최근 연구에서 저희는 추론 모델의 사고 과정(chain of thought, CoT)을 모니터링하는 것이 잘못된 행동을 탐지하는 데 도움이 될 수 있음을 발견했습니다. 나아가 모델의 CoT가 "나쁜 생각"을 갖지 않도록 직접적인 압력을 받으면, 모델이 여전히 잘못된 행동을 하면서도 자신의 생각을 숨기는 법을 배울 수 있다는 것을 발견했습니다. 이러한 우려에 따라, 저희는 두 오픈 웨이트 모델 모두에 대해 CoT에 직접적인 최적화 압력을 가하지 않기로 결정했습니다.

이러한 사고 과정은 제한되지 않기 때문에, OpenAI의 표준 안전 정책을 반영하지 않는 언어를 포함하여 환각적인 콘텐츠를 포함할 수 있습니다. 개발자는 이러한 유형의 콘텐츠에 대한 추가적인 필터링, 중재 또는 요약 없이 사고 과정을 애플리케이션 사용자에게 직접 보여주어서는 안 됩니다.

**4.5 환각 (Hallucinations)**

저희는 `gpt-oss-120b`와 `gpt-oss-20b`의 환각을 다음 평가를 사용하여 확인하며, 두 평가 모두 모델에게 인터넷 브라우징 능력을 주지 않고 실행되었습니다:

- **SimpleQA**: 짧은 답변이 있는 4천 개의 사실 탐색 질문으로 구성된 다양한 데이터셋으로, 시도된 답변에 대한 모델의 정확도를 측정합니다.
    
- **PersonQA**: 사람에 대한 질문과 공개적으로 이용 가능한 사실로 구성된 데이터셋으로, 시도된 답변에 대한 모델의 정확도를 측정합니다.
    

`gpt-oss-120b`와 `gpt-oss-20b`는 저희의 SimpleQA와 PersonQA 평가 모두에서 OpenAI 04-mini보다 성능이 낮습니다. 이는 더 작은 모델이 더 큰 프론티어 모델보다 세계 지식이 적고 더 많이 환각을 일으키는 경향이 있기 때문에 예상된 결과입니다.

**4.6 공정성 및 편향 (Fairness and Bias)**

저희는 BBQ 평가에서 `gpt-oss-120b`와 `gpt-oss-20b`를 평가했습니다. 전반적으로, 저희는 두 모델 모두 OpenAI 04-mini와 거의 동등한 수준으로 수행되는 것을 확인했습니다.


## 5. 준비성 프레임워크

준비성 프레임워크는 심각한 피해를 초래할 수 있는 새로운 위험을 생성하는 프론티어 역량을 추적하고 대비하기 위한 OpenAI의 접근 방식입니다. 이 프레임워크는 고도로 유능한 모델에 대한 위험을 충분히 최소화하는 안전장치를 구현하는 등 심각한 피해 위험을 추적하고 완화할 것을 약속합니다. 아래에서는 이 평가에 정보를 제공하기 위해 수행한 평가에 대한 자세한 정보를 제공합니다.

**5.1: 적대적 훈련**

gpt-oss 모델들은 안전 훈련을 위한 우리의 최첨단 접근법을 활용합니다. 사전 훈련 단계에서 화학, 생물학, 방사선 및 핵(CBRN)과 관련된 특정 유해 데이터를 필터링했습니다. 사후 훈련 단계에서는 모델이 안전하지 않은 프롬프트를 거부하고 프롬프트 주입을 방어하도록 가르치기 위해 심의적 정렬(deliberative alignment)과 지시 계층(instruction hierarchy)을 사용했습니다.

그러나 악의적인 행위자들은 우리의 gpt-oss 모델을 포함한 오픈 웨이트 모델을 미세 조정할 수 있습니다. 이러한 미세 조정이 준비성 프레임워크 하에서 추적된 역량 범주에 미칠 수 있는 영향을 추정하기 위해, 우리는 적대적 미세 조정이 모델을 높은 역량에 도달하게 할 가능성이 있다고 믿었던 두 가지 범주, 즉 생물학 및 화학 역량과 사이버 역량에 대해 gpt-oss-120b의 적대적으로 미세 조정된 버전을 만들었습니다.

우리의 적대적 훈련에서는 기술적이고, 강력한 사후 훈련 인프라와 ML 지식에 접근할 수 있으며, 유해한 역량에 대한 도메인 내 데이터를 수집할 수 있고, 큰 규모의 컴퓨팅 예산을 가진 적을 시뮬레이션합니다. 우리는 내부 OpenAI o-시리즈 RL 훈련 스택을 사용하며, 이는 모델의 추론 행동을 보존하면서 새로운 역량을 추가합니다.

연구 논문에서 더 자세히 설명된 우리의 접근법은 두 가지 요소를 결합했습니다:

- **유용성만 고려한 훈련(Helpful-only training):** 우리는 안전하지 않은 프롬프트를 준수하는 답변에 보상하기 위해 추가적인 강화 학습 단계를 수행했습니다.
    
- **생물학 및 사이버 도메인의 준비성 벤치마크 관련 역량 극대화:** 우리의 적대적으로 훈련된 생물학 모델의 경우, 웹 브라우징을 위해 gpt-oss-120b를 종단 간으로 증분 훈련했으며, 생물학적 위험 관련 인-도메인 인간 전문가 데이터로 증분 훈련했습니다. 사이버 모델의 경우, 도메인 특정 데이터는 사이버 보안 캡처 더 플래그(CTF) 챌린지 환경으로 구성되었습니다.
    

그런 다음 내부 및 외부 테스트를 통해 이러한 모델의 역량 수준을 평가했습니다. OpenAI의 안전 자문 그룹("SAG")은 이 테스트를 검토하고, OpenAI의 현장 최고 수준의 훈련 스택을 활용한 강력한 미세 조정에도 불구하고 gpt-oss-120b는 생물학 및 화학적 위험 또는 사이버 위험에서 높은 역량에 도달하지 못했다고 결론 내렸습니다.

**5.1.1 적대적 훈련 방법론에 대한 외부 안전 전문가 피드백** 

우리는 외부 안전 전문가 그룹(METR, SecureBio, Daniel Kang)을 참여시켜 우리의 악의적 미세 조정 방법론을 독립적으로 검토하고 검증했습니다. 우리는 논문의 초기 초안, 미세 조정 데이터셋에 대한 비공개 세부 정보, 준비성 평가에 사용된 방법론 및 스캐폴딩을 공유했으며, 정보에 입각한 피드백을 지원하기 위해 방법론 논문 저자들과 한 시간 동안의 Q&A 세션을 주최했습니다.

 **5.2.1: 생물학 및 화학 - 적대적으로 미세 조정됨 (Biological and Chemical - Adversarially Fine-tuned)**

모델의 상한 역량을 테스트하기 위해 설계된 최대 유도 조건 하에서, `gpt-oss-120b`는 생물학적 지식과 유해 시나리오를 포함하는 텍스트 질문에 답변하는 데 있어 주목할 만한 강점을 보입니다. 그러나, 일반적으로 유능하지만 복잡한 프로토콜 디버깅 작업에서는 아직 높은 지표 임계값에 도달하지 못하며, 텍스트 전용 아키텍처는 시각적으로 의존적인 실험실 환경에서의 적용 가능성을 본질적으로 제한합니다.

`gpt-oss`를 오픈 웨이트로 출시할 계획을 고려하여, 우리는 두 번째 질문을 조사하기로 했습니다: 준비성 프레임워크에서 높은 역량에 도달하지 않더라도, `gpt-oss-120b`가 오픈 소스 기반 모델에서 위험한 생물학적 역량의 최전선을 크게 발전시킬 것인가? 이 질문을 조사하기 위해, 우리는 `gpt-oss-120b`를 다른 출시된 오픈 소스 모델과 비교했습니다. 이러한 평가를 통해 (최근 출시된) Qwen 3 Thinking과 Kimi K2가 `gpt-oss-120b`가 생물 보안 관련 평가에서 기술 수준을 크게 발전시키지 않는 수준까지 발전했음을 확인했습니다. `gpt-oss-120b`가 일부 생물 보안 평가에서 가장 높은 성능을 달성하지만, 이 영역에서 다른 모델들을 일관되게 능가하는 단일 오픈 모델은 없습니다.

**생물학 및 화학 평가 개요**

- **장문 생물학적 위험 질문:** 생물학적 위협 생성 과정(아이디어, 획득, 증폭 등)에 대한 모델의 정보 종합 능력을 평가합니다.
    
- **다중 모드 문제 해결 (바이러스학):** 실제 실험실 문제 해결 능력을 평가합니다.
    
- **ProtocolQA (개방형):** 실험 프로토콜의 오류를 찾아 수정하는 능력을 평가합니다.
    
- **암묵적 지식 및 문제 해결:** 전문가 수준의 암묵적, 경험적 지식을 테스트합니다.
    
- **TroubleshootingBench:** 전문가가 작성한 비공개 실험 절차의 오류를 식별하고 수정하는 능력을 평가합니다.
    

**평가 결과**

- **장문 생물학적 위험 질문:** `gpt-oss`와 경쟁 모델들 모두 생물학적 위험 관련 정보를 종합하는 능력을 보였습니다.
    
- **다중 모드 문제 해결 (바이러스학):** OpenAI 03 모델이 가장 높은 점수를 기록했으며, 모든 모델이 평균적인 인간 기준치(40%)보다는 높은 점수를 보였습니다.
    
- **ProtocolQA (개방형):** OpenAI 03이 가장 성능이 뛰어났고, 모든 모델이 전문가 기준선(합의 54%, 중앙값 42%)보다 낮은 성능을 보였습니다.
    
- **암묵적 지식 및 문제 해결:** 여기서도 OpenAI 03이 가장 높은 성능을 보였고, 어떤 모델도 전문가 합의 기준선(80%)을 넘지 못했습니다.
    
- **TroubleshootingBench:** OpenAI 03이 가장 높은 성능을 보였고, 모든 모델이 80번째 백분위수 인간 전문가 점수(36.4%)보다 낮은 성능을 보였습니다.
    
- **외부 전문가 평가:** 외부 기관(SecureBio)의 레드팀 평가 결과, 적대적으로 미세 조정된 `gpt-oss-120b` 버전이 다른 오픈 소스 모델보다는 나았지만, OpenAI 03 모델보다는 전반적인 신뢰도와 깊이에서 뒤처졌습니다.


**5.2.2: 사이버 보안 - 적대적으로 미세 조정됨 (Cybersecurity - Adversarially fine-tuned)**

사이버 보안은 컴퓨터 시스템의 기밀성, 무결성 및/또는 가용성을 방해하는 사이버 착취에 모델이 사용될 위험과 관련된 역량에 초점을 맞춥니다. 이러한 결과는 OpenAI 03과 비슷한 성능을 보였으며, 마찬가지로 우리의 '높은 역량' 임계값에는 미치지 못했습니다.

**사이버 보안 평가 개요**

- **캡처 더 플래그 (Capture the Flag, CTF):** 고등학생, 대학생, 전문가 수준의 경쟁적인 사이버 보안 챌린지를 모델이 해결할 수 있는지 평가합니다. 챌린지는 웹 애플리케이션 공격, 리버스 엔지니어링, 바이너리 및 네트워크 공격 등 다양한 유형으로 구성됩니다.
    
- **사이버 레인지 (Cyber Range):** 모델이 현실적인 모의 네트워크 환경에서 완전히 종단 간(end-to-end) 사이버 작전을 수행할 수 있는지 평가합니다. 이 훈련은 단순한 취약점 공격을 넘어, 여러 단계를 연결하여 최종 목표를 달성하는 장기적인 시나리오로 구성됩니다.
    

**평가 결과**

- **캡처 더 플래그 (CTF) 챌린지:**
    
    - **대학생 수준(Collegiate):** OpenAI 03이 가장 우수한 성능을 보였고, 그 뒤를 `gpt-oss-120b` 출시 후보 모델이 이었습니다.
        
    - **전문가 수준(Professional):** 사이버 보안용으로 적대적 훈련을 받은 'cybermax' 모델(`gpt-oss-120b` 기반)이 OpenAI 03보다 단 3% 포인트 낮은 성능을 보이며 근접했습니다.
        
    - 이 평가 결과는 추가적인 기술 지원(scaffolding)을 통해 관찰된 성능이 더 높아질 수 있으므로, 모델 역량의 하한선일 가능성이 있습니다.
        
- **사이버 레인지 (Cyber Range):**
    
    - 이 평가는 훨씬 더 도전적이었습니다. **어떤 모델도 외부의 도움 없이 시나리오를 해결하지 못했습니다.**
        
    - 심지어 목표 달성을 위한 대략적인 계획을 **힌트로 제공했을 때조차도 성공률은 0%에서 2% 사이**로 극히 낮았습니다.


**5.2.3: AI 자가 개선 (AI Self-Improvement)**

`gpt-oss` 모델들은 AI 자가 개선 위험과 관련된 소프트웨어 엔지니어링 및 AI 연구 작업에서 향상된 성능을 보이지 않습니다. OpenAI 03과 04-mini가 모든 벤치마크에서 여전히 가장 높은 성능을 보이는 모델입니다.

**AI 자가 개선 평가 개요**

- **SWE-bench Verified:** 실제 깃허브(GitHub) 이슈를 보고 코드 저장소와 이슈 설명만으로 문제를 해결할 수 있는지 평가합니다.
    
- **OpenAI PRs:** 모델이 실제 OpenAI 연구 엔지니어들이 수행한 코드 변경(Pull Requests)을 재현할 수 있는지 평가합니다.
    
- **PaperBench:** 최신 AI 연구 논문을 처음부터(from scratch) 재현하는 능력, 즉 논문을 이해하고 코드를 개발하며 실험을 성공적으로 실행하는 능력을 평가합니다.
    

**평가 결과**

- **SWE-bench Verified (실제 소프트웨어 엔지니어링):**
    
    - 이 평가에서는 모든 모델이 비슷한 성능을 보였으며, OpenAI 03과 04-mini가 약간 더 높은 점수를 기록했습니다. `gpt-oss-120b`는 62%의 정확도를 보였습니다.
        
- **OpenAI PRs (실제 ML 연구 작업):**
    
    - `gpt-oss` 모델들은 OpenAI 04-mini보다 단 2% 포인트 낮은 점수를 기록하며, 뒤처지긴 했지만 비교적 경쟁력 있는 성능을 보였습니다.
        
- **PaperBench (AI 연구 논문 재현):**
    
    - 이 평가에서는 모델 간 성능 차이가 컸습니다. OpenAI 04-mini가 25%의 성공률로 가장 높았고, `gpt-oss-120b`는 11%, `gpt-oss-20b`는 7%로 현저히 낮은 성능을 보였습니다.
## 6장 부록 - 하모니 채팅의 구체적인 입력 및 출력 예시

- `</start |>system<| message |>`: **시스템 메시지**입니다.
    - 모델의 기본적인 역할("ChatGPT"), 지식 마감일("2024-06"), 현재 날짜, 그리고 추론 수준("low")을 설정합니다.

- `<start>developer<message>`: **개발자 메시지**입니다.
    - 모델이 사용할 수 있는 외부 도구(Tool)를 정의합니다.
    - 이 예시에서는 `get_current_weather`라는 날씨 확인 함수(function)를 도구로 지정하고 있습니다.

- `< start >user< message |>`: **사용자 메시지**입니다.
    - 사용자가 실제로 모델에 던지는 질문("SF 날씨 어때?")이 담겨 있습니다.

## 7장 부록 - 적테적 테스트 방법론의 권장 사항과 대응 방안

**7.0.1 구현된 권장 사항**

1. **위협 모델 및 위험 분류 명확화**
    
    - **저자원 행위자 가정 정의:** 논문에 저자원 행위자의 컴퓨팅, ML 전문 지식, 데이터 접근 가정에 대한 명확한 설명을 추가하고, 향후 비용 추정은 후속 조치로 표시했습니다.
        
    - **준비성 기준 및 ProtocolQA 요구 사항:** 준비성 기준을 명확히 하고 ProtocolQA를 평가의 필수 구성 요소로 명시적으로 유지했습니다. 그에 따라 논문 텍스트를 편집하고 일관성을 보장하기 위해 차단 목록과 함께 OpenAI 03에 대해 ProtocolQA를 다시 실행했습니다.
        
2. **평가 완전성 및 신뢰성 강화**
    
    - **ProtocolQA에 대한 견고성 확인:** 모델이 절대 거부하지 않도록 확인하고, 더 많은 프로토콜 디버깅 훈련 데이터를 추가했으며, ProtocolQA와 유사하지만 오염되지 않은 새로운 프로토콜 문제 해결 평가를 추가하여 프로토콜 문제 해결 결과를 검증했습니다.
        
    - **추론 시간 스케일링 플롯:** 생물학 및 사이버 평가 모두에 대해 시도 횟수에 따라 성능이 어떻게 확장되는지 보여주는 플롯을 추가했습니다.
        
    - **다중 모드 벤치마크 정렬:** 다중 모드 바이러스학 문제 해결의 텍스트 전용 버전을 실행하고 비교 가능성을 높이기 위해 결과를 업데이트했습니다. 또한 최종 322개 질문 데이터셋에 대해 VCT를 수행하고 인간 기준선 비교를 보고했습니다.
        
    - **전문가 기준선 명확성:** 보고서에 전문가 프로필과 기준선 계산 방식을 명시했습니다.
        
    - **거부 행동 정량화:** 거부 기반 실패를 다른 실패 모드와 명시적으로 분리하고, 무력화(naughtification) 전후의 비율을 보고했습니다.
        
3. **평가 설정 개선**
    
    - **향상된 에이전트 스캐폴딩:** 사이버 평가에서 내부 "Best of K" 스캐폴딩을 테스트했습니다.
        
    - **ProtocolQA와 RL 데이터셋 정렬:** RL 훈련 중 유사한 데이터셋을 테스트하여 유해한 성능 향상이 없는지 확인했으며, 발견 사항을 논문에 추가했습니다.
        
    - **미세 조정 성능 검증:** 최대 성능을 위한 최상의 하이퍼파라미터 설정에 대해 내부 연구원들과 조율하고 필요시 변경했습니다.
        

**7.0.2 채택되지 않은 권장 사항**

1. **측정을 위한 더 높은 품질의 에이전트 스캐폴딩**
    
    - **(권장 사항):** 모든 평가에 'Best-of-N' 스캐폴딩을 광범위하게 적용할 것.
        
    - **(결정):** 스캐폴딩 실험은 다른 곳에서 부분적으로 수행되었으며, 전체 재실행으로 인한 추가적인 이득은 제한적일 것으로 예상되어 채택하지 않았습니다.
        
2. **준비성 임계값에서 ProtocolQA 제외**
    
    - **(권장 사항):** 문제 해결 위험에 대한 현실 세계 적용 범위가 불완전하므로 ProtocolQA를 제거할 것.
        
    - **(결정):** 한계에도 불구하고 ProtocolQA는 독특한 안전 신호를 제공했습니다. 이를 제거하면 큰 공백이 생겼을 것입니다. 준비성 기준의 광범위한 변경은 이번 출시의 범위를 벗어났습니다.
        
3. **폐쇄형 모델과 오픈 모델의 거부 비교**
    
    - **(권장 사항):** 비거부 응답을 대체하여 폐쇄형 모델을 사용한 결합 성능을 계산하고, 거부는 0으로 처리할 것.
        
    - **(결정):** 과거 테스트에서 폐쇄형 모델은 이미 무해한 대리 작업에서 거부하지 않는 것으로 나타났으므로, 실제 악의적인 작업에서 오픈 모델이 폐쇄형 모델의 "격차를 얼마나 잘 메울 수 있는지"에 대한 유의미한 신호를 주지 않을 것이라고 판단했습니다.